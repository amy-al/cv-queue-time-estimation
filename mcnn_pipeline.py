# -*- coding: utf-8 -*-
"""mcnn_pipeline

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PgL13HvuS0xq1HjFXtb3CzrZ6hbKTZkG
"""

### Portions of this code was provided by Mehreen Tahir. The source code can be found here: https://www.codeproject.com/Articles/5283660/AI-Queue-Length-Detection-Counting-the-Number-of-P

from google.colab import drive
drive.mount('/content/drive')

"""### TRAINED MODEL"""

import os
import cv2
import csv
import math
import random
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from keras import models
from keras import backend as K
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from keras.models import load_model, Model
from keras.callbacks import ModelCheckpoint
from keras.layers import Conv2D, MaxPooling2D, Concatenate, Input

# density function
def get_density_map(image, points):

    image_density = np.zeros_like(image, dtype=np.float64)
    height, width = image_density.shape
    if points is None:
        return image_density
    if points.shape[0] == 1:
        x1 = max(0, min(width-1, round(points[0, 0])))
        y1 = max(0, min(height-1, round(points[0, 1])))
        image_density[y1, x1] = 255
        return image_density
    for j in range(points.shape[0]):
        frame_size = 15
        sigma = 4.0
        Height = np.multiply(cv2.getGaussianKernel(frame_size, sigma), (cv2.getGaussianKernel(frame_size, sigma)).T)
        x = min(width-1, max(0, abs(int(math.floor(points[j, 0])))))
        y = min(height-1, max(0, abs(int(math.floor(points[j, 1])))))
        if x >= width or y >= height:
            continue
        x1 = x - frame_size//2 + 0
        y1 = y - frame_size//2 + 0
        x2 = x + frame_size//2 + 1
        y2 = y + frame_size//2 + 1
        dfx1, dfy1, dfx2, dfy2 = 0, 0, 0, 0
        change_Height = False
        if x1 < 0:
            dfx1 = abs(x1) + 0
            x1 = 0
            change_Height = True
        if y1 < 0:
            dfy1 = abs(y1) + 0
            y1 = 0
            change_Height = True
        if x2 > width:
            dfx2 = x2 - width
            x2 = width
            change_Height = True
        if y2 > height:
            dfy2 = y2 - height
            y2 = height
            change_Height = True
        x1h, y1h, x2h, y2h = 1 + dfx1, 1 + dfy1, frame_size - dfx2, frame_size - dfy2
        if change_Height is True:
            Height = np.multiply(cv2.getGaussianKernel(y2h-y1h+1, sigma), (cv2.getGaussianKernel(x2h-x1h+1, sigma)).T)
        image_density[y1:y2, x1:x2] += Height

    return image_density

# test data processing

# images_path = ''.join(['/content/drive/My Drive/ShanghaiTech/part_B/test_data/images/'])
# ground_truth_path = ''.join(['/content/drive/My Drive/ShanghaiTech/part_B/test_data/ground-truth/'])
# ground_truth_csv = ''.join(['/content/drive/My Drive/ShanghaiTech/part_B/test_data/ground-truth_csv/'])

# n = 316

# input_images_path = ''.join(['/content/drive/My Drive/ShanghaiTech/part_B/train_data/images/'])
# output_path = '/content/drive/My Drive/ShanghaiTech/processed_trainval/'

# training_images_path = ''.join((output_path, '/training_images/'))
# training_densities_path = ''.join((output_path, '/training_densities/'))
# validation_images_path = ''.join((output_path, '/validation_images/'))
# validation_densities_path = ''.join((output_path, '/valalidation_densities/'))

# ground_truth_path = ''.join(['/content/drive/My Drive/ShanghaiTech/part_B/train_data/ground-truth/'])

# for i in [output_path, training_images_path, training_densities_path, validation_images_path, validation_densities_path]:
#     if not os.path.exists(i):
#         os.makedirs(i)

training_images = '/content/drive/My Drive/ShanghaiTech/processed_trainval/training_images/'
training_densities = '/content/drive/My Drive/ShanghaiTech/processed_trainval/training_densities/'
validation_images = '/content/drive/My Drive/ShanghaiTech/processed_trainval/validation_images/'
validation_densities = '/content/drive/My Drive/ShanghaiTech/processed_trainval/valalidation_densities/'
test_images = '/content/drive/My Drive/ShanghaiTech/part_B/test_data/images/'
test_densities = '/content/drive/My Drive/ShanghaiTech/part_B/test_data/ground-truth_csv/'

train_paths = sorted([training_images + p for p in os.listdir(training_images)])
train_labels = sorted([training_densities + p for p in os.listdir(training_densities)])
validation_paths = sorted([validation_images + p for p in os.listdir(validation_images)])
validation_labels = sorted([validation_densities + p for p in os.listdir(validation_densities)])
test_paths = sorted([test_images + p for p in os.listdir(test_images)])
test_labels = sorted([test_densities + p for p in os.listdir(test_densities)])

def x_y_generator(images_path, labels_path, batch_size=64):
    break_point = 0
    t = 0
    images_path = np.squeeze(images_path).tolist() if isinstance(images_path, np.ndarray) else images_path
    labels_path = np.squeeze(labels_path).tolist() if isinstance(labels_path, np.ndarray) else labels_path
    data_length = len(labels_path)
    while True:
        if not break_point:
            x = []
            y = []
            inner_iteration = batch_size
        else:
            t = 0
            inner_iteration = batch_size - data_length % batch_size
        for i in range(inner_iteration):
            if t >= data_length:
                break_point = 1
                break
            else:
                break_point = 0
            img = (cv2.imread(images_path[t], 0) - 127.5) / 128
            density_map = np.loadtxt(labels_path[t], delimiter=',')
            std = 4
            quarter_den = np.zeros((np.asarray(density_map.shape).astype(int)//std).tolist())
            for r in range(quarter_den.shape[0]):
                for c in range(quarter_den.shape[1]):
                    quarter_den[r, c] = np.sum(density_map[r*std:(r+1)*std, c*std:(c+1)*std])
            x.append(img.reshape(*img.shape, 1))
            y.append(quarter_den.reshape(*quarter_den.shape, 1))
            t += 1
        if not break_point:
            x, y = np.asarray(x), np.asarray(y)
            yield x, y

def mean_absolute_error(labels, predictions):
    return K.sum(K.abs(labels - predictions)) / 1


def mean_square_error(labels, predictions):
    return K.sum(K.square(labels - predictions)) / 1

direx = '/content/drive/My Drive/ShanghaiTech/weights'

def non_max_suppression(density_map, neighbourhood):
  # get the shape of the density map
  height, width = density_map.shape
  # create a new array to store the output
  output = np.zeros_like(density_map)
  # loop over each pixel in the density map
  for i in range(height):
    for j in range(width):
      # get the value of the current pixel
      value = density_map[i, j]
      # get the coordinates of the neighbourhood window
      x1 = max(0, i - neighbourhood // 2)
      y1 = max(0, j - neighbourhood // 2)
      x2 = min(height, i + neighbourhood // 2 + 1)
      y2 = min(width, j + neighbourhood // 2 + 1)
      # get the maximum value in the neighbourhood window
      max_value = np.max(density_map[x1:x2, y1:y2])
      # if the current pixel has the maximum value, copy it to the output
      if value == max_value:
        output[i, j] = value
  # return the output array
  return output

"""### OUTLIER REMOVAL FUNCTIONS"""

def index_closest(lst, x):
    return min(range(len(lst)), key=lambda i: abs(lst[i] - x))

def polynomial_regression(x, y, degree, threshold):
    """
    Calculate and plot the polynomial regression for a set of points, along with threshold lines.

    Parameters:
    x (list or np.array): x-coordinates of the points
    y (list or np.array): y-coordinates of the points
    degree (int): Degree of the polynomial
    threshold (float): The threshold distance from the regression line
    """

    count=0
    x = np.array(x)
    y = np.array(y)

    coefficients = np.polyfit(x, y, degree)

    # Generate a range of x values for plotting the polynomial line and threshold lines
    x_fit = np.linspace(min(x), max(x), 500)
    y_fit = np.polyval(coefficients, x_fit)

    # threshold lines
    y_fit_upper = y_fit + threshold
    y_fit_lower = y_fit - threshold

    for xx,yy in zip(x,y):
        x1=index_closest(x_fit,xx)
        y1_up=y_fit_upper[x1]
        y1_low=y_fit_lower[x1]

        if y1_low<yy<y1_up:
            count+=1

    # plt.figure(figsize=(10, 6))
    # plt.scatter(x, y, color='blue', label='Data')
    # plt.plot(x_fit, y_fit, color='red', label=f'Polynomial Regression/ Degree =  {degree}')
    # plt.plot(x_fit, y_fit_upper, color='green', linestyle='--', label=f'Upper threshold (+{threshold})')
    # plt.plot(x_fit, y_fit_lower, color='purple', linestyle='--', label=f'Lower threshold (-{threshold})')
    # plt.xlabel('X')
    # plt.ylabel('Y')
    # plt.title(f'Polynomial Regression of the queue with thresholds, degree= {degree} ')
    # plt.legend()
    # plt.show()

    return count

"""### MAIN"""

# PARAMETERS MUST BE SET
queue_image_path1 = "/path/to/queue_image1.jpg"
queue_image_path2 = "/path/to/queue_image2.jpg"
time_difference = 1 # in minutes
polynomial_degree = 5
threshold = 3.9

img1 = (cv2.imread(queue_image_path1, 0) - 127.5) / 128
img2 = (cv2.imread(queue_image_path2, 0) - 127.5) / 128

queue_images = [img1, img2]

model = models.load_model(os.path.join(direx, 'mcnn_val_epoch_200.hdf5'), custom_objects={'mean_absolute_error': mean_absolute_error, 'mean_square_error': mean_square_error})
absolute_error = []
square_error = []
num_test = 2

prev_count = 0

for i in range(2):
    inputs = np.reshape(queue_images[i], [1, *queue_images[i].shape[:2], 1])
    outputs = np.squeeze(model.predict(inputs))

    image_shape_x, image_shape_y = queue_images[i].shape[:2]
    output_shape_x, output_shape_y = outputs.shape
    ratio_x = image_shape_x / output_shape_x
    ratio_y = image_shape_y / output_shape_y

    count_label = 0
    count_prediction = np.sum(outputs)
    fg, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 5))
    plt.suptitle(' '.join([
        'count_label:', str(round(count_label, 3)),
        'count_prediction:', str(round(count_prediction, 3))
    ]))
    ax0.imshow(np.squeeze(inputs), cmap='gray')
    ax1.imshow(outputs * (255 / (np.max(outputs) - np.min(outputs))), cmap='gray')

    plt.show()

    coords = np.argwhere(outputs > 0)

    x_coords = []
    y_coords = []
    for coord in coords:
      x, y = coord
      x_coords.append(x)
      y_coords.append(y)

    x_coords = np.array(x_coords)
    y_coords = np.array(y_coords)

    if i == 0:
      count1 = polynomial_regression(x_coords, y_coords,threshold, polynomial_degree)
    else:
      count2 = polynomial_regression(x_coords, y_coords,threshold, polynomial_degree)

count_difference = count2 - count1
if count_difference < 0:
    print("Error: there are more people in the second picture than the first. Make sure the order is correct and that you have waited 5 mins before taking another picture.")

time_prediction = count2 * (count_difference/time_difference)